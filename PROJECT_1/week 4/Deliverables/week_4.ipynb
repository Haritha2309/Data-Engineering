{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSC5RV2qdKHi",
        "outputId": "d3f24a07-983f-4389-c35e-63ee609f711f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install delta-spark==3.2.0 -q\n",
        "import pyspark\n",
        "from delta import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a SparkSession with Delta Lake extensions\n",
        "# The '.config(...)' lines are crucial for enabling Delta Lake's features\n",
        "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaTutorial\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "# Get or create the SparkSession\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "print(\"Spark and Delta Lake are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sYSET32mkL9",
        "outputId": "9c5e3393-504c-4b18-af0f-56775f86b7fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark and Delta Lake are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADED CLEANED FILE\n",
        "file_path = \"/content/drive/MyDrive/data engineering/Week 4/cleaned_students_progress.csv\"\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
        "\n",
        "df.show()\n",
        "\n",
        "# LOADING STUDENTS FILE\n",
        "file_path_s = \"/content/drive/MyDrive/data engineering/Week 4/students.csv\"\n",
        "\n",
        "df_s = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path_s)\n",
        "\n",
        "df_s.show()\n",
        "\n",
        "# LOADING COURSE FILE\n",
        "file_path_c = \"/content/drive/MyDrive/data engineering/Week 4/courses.csv\"\n",
        "\n",
        "df_c = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path_c)\n",
        "\n",
        "df_c.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ci85g4ZgTbo",
        "outputId": "b41e2e98-4743-4224-b7cc-2c3791d6d6d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+---------------------+------------+----------+---------+---------------+\n",
            "|progress_id|enrollment_id|completion_percentage|last_updated|student_id|course_id|enrollment_date|\n",
            "+-----------+-------------+---------------------+------------+----------+---------+---------------+\n",
            "|          1|            1|                 85.0|  2024-02-01|         1|      101|     2024-01-10|\n",
            "|          2|            2|                 50.0|  2024-02-02|         2|      102|     2024-01-30|\n",
            "|          3|            3|                 50.0|  2024-02-03|         3|      103|     2024-01-15|\n",
            "|          4|            4|                100.0|  2024-02-15|         4|      101|     2024-01-18|\n",
            "|          5|            5|                 70.0|  2024-02-05|         5|      102|     2024-01-20|\n",
            "+-----------+-------------+---------------------+------------+----------+---------+---------------+\n",
            "\n",
            "+----------+------------+----------------+---------------+\n",
            "|student_id|student_name|   student_email|student_college|\n",
            "+----------+------------+----------------+---------------+\n",
            "|         1|        AMIT|  amit@gmail.com|            KCT|\n",
            "|         2|       ROHIT| rohit@gmail.com|           MCET|\n",
            "|         3|      LOKESH|lokesh@gmail.com|            KPR|\n",
            "|         4|       RAMYA| ramya@gmail.com|           SKET|\n",
            "|         5|       PRIYA| priya@gmail.com|           MCET|\n",
            "+----------+------------+----------------+---------------+\n",
            "\n",
            "+---------+-----------+-----------------------+--------+\n",
            "|course_id|course_name|description            |faculty |\n",
            "+---------+-----------+-----------------------+--------+\n",
            "|101      |FSD        |website development    |HARI    |\n",
            "|102      |EMBEDDED   |hardware + software    |KARPAGAM|\n",
            "|103      |ML         |machine learning basics|RAJ     |\n",
            "+---------+-----------+-----------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join progress + students\n",
        "merged_df = df.join(df_s, on=\"student_id\", how=\"inner\")\n",
        "merged_df.show()\n",
        "\n",
        "# Join with courses\n",
        "final_df = merged_df.join(df_c, on=\"course_id\", how=\"inner\")\n",
        "print(\"------------\\n\")\n",
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChHvehG1nW9d",
        "outputId": "59e9fc9d-d5bf-4f63-ea7e-61e4fd6a7719"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------------+---------------------+------------+---------+---------------+------------+----------------+---------------+\n",
            "|student_id|progress_id|enrollment_id|completion_percentage|last_updated|course_id|enrollment_date|student_name|   student_email|student_college|\n",
            "+----------+-----------+-------------+---------------------+------------+---------+---------------+------------+----------------+---------------+\n",
            "|         1|          1|            1|                 85.0|  2024-02-01|      101|     2024-01-10|        AMIT|  amit@gmail.com|            KCT|\n",
            "|         2|          2|            2|                 50.0|  2024-02-02|      102|     2024-01-30|       ROHIT| rohit@gmail.com|           MCET|\n",
            "|         3|          3|            3|                 50.0|  2024-02-03|      103|     2024-01-15|      LOKESH|lokesh@gmail.com|            KPR|\n",
            "|         4|          4|            4|                100.0|  2024-02-15|      101|     2024-01-18|       RAMYA| ramya@gmail.com|           SKET|\n",
            "|         5|          5|            5|                 70.0|  2024-02-05|      102|     2024-01-20|       PRIYA| priya@gmail.com|           MCET|\n",
            "+----------+-----------+-------------+---------------------+------------+---------+---------------+------------+----------------+---------------+\n",
            "\n",
            "------------\n",
            "\n",
            "+---------+----------+-----------+-------------+---------------------+------------+---------------+------------+----------------+---------------+-----------+--------------------+--------+\n",
            "|course_id|student_id|progress_id|enrollment_id|completion_percentage|last_updated|enrollment_date|student_name|   student_email|student_college|course_name|         description| faculty|\n",
            "+---------+----------+-----------+-------------+---------------------+------------+---------------+------------+----------------+---------------+-----------+--------------------+--------+\n",
            "|      101|         1|          1|            1|                 85.0|  2024-02-01|     2024-01-10|        AMIT|  amit@gmail.com|            KCT|        FSD| website development|    HARI|\n",
            "|      102|         2|          2|            2|                 50.0|  2024-02-02|     2024-01-30|       ROHIT| rohit@gmail.com|           MCET|   EMBEDDED| hardware + software|KARPAGAM|\n",
            "|      103|         3|          3|            3|                 50.0|  2024-02-03|     2024-01-15|      LOKESH|lokesh@gmail.com|            KPR|         ML|machine learning ...|     RAJ|\n",
            "|      101|         4|          4|            4|                100.0|  2024-02-15|     2024-01-18|       RAMYA| ramya@gmail.com|           SKET|        FSD| website development|    HARI|\n",
            "|      102|         5|          5|            5|                 70.0|  2024-02-05|     2024-01-20|       PRIYA| priya@gmail.com|           MCET|   EMBEDDED| hardware + software|KARPAGAM|\n",
            "+---------+----------+-----------+-------------+---------------------+------------+---------------+------------+----------------+---------------+-----------+--------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df2 = final_df.select(\"student_name\", \"course_name\", \"enrollment_date\", \"completion_percentage\")\n",
        "\n",
        "print(\"------------ FINAL DF -------------\\n\")\n",
        "final_df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VDvIu2goPt8",
        "outputId": "da7477dd-fad9-4b0b-b930-8126810252a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ FINAL DF -------------\n",
            "\n",
            "+------------+-----------+---------------+---------------------+\n",
            "|student_name|course_name|enrollment_date|completion_percentage|\n",
            "+------------+-----------+---------------+---------------------+\n",
            "|AMIT        |FSD        |2024-01-10     |85.0                 |\n",
            "|ROHIT       |EMBEDDED   |2024-01-30     |50.0                 |\n",
            "|LOKESH      |ML         |2024-01-15     |50.0                 |\n",
            "|RAMYA       |FSD        |2024-01-18     |100.0                |\n",
            "|PRIYA       |EMBEDDED   |2024-01-20     |70.0                 |\n",
            "+------------+-----------+---------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_csv_path = \"/content/drive/MyDrive/data engineering/Week 4/final_df2.csv\"\n",
        "drive_delta_path = \"/content/drive/MyDrive/data engineering/Week 4/final_df2_delta\"\n",
        "\n",
        "# Save as CSV\n",
        "final_df2.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(drive_csv_path)\n",
        "\n",
        "# Save as Delta\n",
        "final_df2.write.format(\"delta\").mode(\"overwrite\").save(drive_delta_path)\n"
      ],
      "metadata": {
        "id": "MgtHrp1PpPFi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import glob, shutil\n",
        "\n",
        "# Path where Spark saved the CSV folder\n",
        "drive_csv_path = \"/content/drive/MyDrive/data engineering/Week 4/final_df2.csv\"\n",
        "\n",
        "# Find the actual part file inside\n",
        "csv_files = glob.glob(drive_csv_path + \"/part-*.csv\")\n",
        "\n",
        "# Copy to Colab local storage with a nice name\n",
        "local_csv = \"/content/final_df2.csv\"\n",
        "shutil.copy(csv_files[0], local_csv)\n",
        "\n",
        "# Download to your system\n",
        "files.download(local_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vDuWeoGhqhVc",
        "outputId": "5274953d-a66a-4d8d-db15-191acb4a883c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a9caa595-f7e2-41d8-9a22-40899f321fb7\", \"final_df2.csv\", 203)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}